{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = pd.read_csv('data/Traffic_Accidents__2019_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's convert the Date and Time column to the datetime type. To save time, we can specify the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents['Date and Time'] = pd.to_datetime(accidents['Date and Time'], format = '%m/%d/%Y %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have it in datetime format it becomes easy to answer questions like \"How many accidents were there per month?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accidents\n",
    " .assign(month = accidents['Date and Time'].dt.month_name())\n",
    " .month\n",
    " .value_counts(sort = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the number of accidents vary by hour of the day.\n",
    "\n",
    "One method we could try is to extract out the date and hour portions and do a `groupby` + `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accidents\n",
    " .assign(date = accidents['Date and Time'].dt.date, \n",
    "         hour = accidents['Date and Time'].dt.hour)     # Create a date and hour column so that we can group\n",
    " .groupby(['date', 'hour'])\n",
    " ['Accident Number']\n",
    " .count()\n",
    " .reset_index()\n",
    " .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big problem with this, which can be seen if you look carefully at the output above. There are no rows for 6:00, 7:00, or 8:00 on January 1. This is because there were no accidents during these hours, so there were no rows to count.\n",
    "\n",
    "A better method (and one that will require less work to pull off) is to use a `Grouper` to group by hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accidents\n",
    " .groupby(pd.Grouper(key = 'Date and Time',     # point it to your datetime column\n",
    "                     freq = '1h',               # How much do you want to group together values?\n",
    "                     origin = 'epoch'           # This will start times at midnight of 1970-01-01. This ensure\n",
    "                                                # This ensures that we are starting our first grouped period on the hour\n",
    "                    ))\n",
    " ['Accident Number']\n",
    " .count()\n",
    " .reset_index()\n",
    " .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Late night on November 11, 2019 [Nashville received a rare November snow](https://fox17.com/news/local/nws-nashville-sees-rare-early-november-snow). Let's investigate to see if we can detect any effect on the number of accidents the following morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, filter down to the following day\n",
    "snow_day = accidents[(accidents['Date and Time'] >= '2019-11-12') & \n",
    "                     (accidents['Date and Time'] < '2019-11-13')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply a grouper to the our snow day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(snow_day\n",
    " .groupby(pd.Grouper(key = 'Date and Time',\n",
    "                     freq = '1h',\n",
    "                     origin = 'epoch'\n",
    "                    ))\n",
    " ['Accident Number']\n",
    " .count()\n",
    " .plot(figsize = (10,5))\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does look like there were quite a few accidents in the morning, but in isolation, it is hard to know if what we are seeing is unusual. Let's do a comparison with the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_grouped = (accidents\n",
    "                     .assign(weekday = accidents['Date and Time'].dt.day_name())\n",
    "                     .query('weekday != \"Saturday\" and weekday != \"Sunday\"')   # remove the weekends\n",
    "                     .groupby(pd.Grouper(key = 'Date and Time',\n",
    "                                         freq = '1h',\n",
    "                                         origin = 'epoch'\n",
    "                                        ))\n",
    " ['Accident Number']\n",
    " .count()\n",
    " .reset_index()       # convert the Date and Time column back to a regular column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accidents_grouped\n",
    " .assign(hour = accidents_grouped['Date and Time'].dt.hour)\n",
    " .groupby('hour')\n",
    " ['Accident Number']\n",
    " .agg(['mean', 'std', 'median','max'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that in the morning, the 7:00 hour is usually the worst, with an average of more than 5 accidents.\n",
    "\n",
    "Looking back at the snow day, we can see that while the morning hours did have a high number of crashes, none of them were the worst that occurred in 2019.\n",
    "\n",
    "However, together the hours or 6, 7, 8, and 9 all had above-average number of crashes. Maybe we can compare this block of time to this block of time across the whole dataset.\n",
    "\n",
    "One way to accomplish this is to change our grouping frequency to 4 hours. Note that we also need to adjust the origin value so that 6:00 - 10:00 get grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_day.groupby(pd.Grouper(key = 'Date and Time',     \n",
    "                                     freq = '4h',               \n",
    "                                     origin = '2018-12-31 02:00:00'  # This will result in the 6:00 AM - 10:00 AM times to be grouped together           \n",
    "                           ))['Date and Time'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also regroup the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_grouped = (accidents\n",
    "                     .assign(weekday = accidents['Date and Time'].dt.day_name())\n",
    "                     .query('weekday != \"Saturday\" and weekday != \"Sunday\"')\n",
    "                     .groupby(pd.Grouper(key = 'Date and Time',     \n",
    "                                     freq = '4h',               \n",
    "                                     origin = '2018-12-31 02:00:00'                                      \n",
    "                                    ))\n",
    " ['Accident Number']\n",
    " .count()\n",
    " .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accidents_grouped\n",
    " .assign(hour = accidents_grouped['Date and Time'].dt.hour)\n",
    " .groupby('hour')\n",
    " ['Accident Number']\n",
    " .agg(['mean', 'std', 'median', 'max'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the snow day to the overall average for the 4 hour period starting at 6:00 AM, we can see that there were an above-average number of accidents. It wasn't the worst day in the whole year, but let's investigate and see where it lands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accidents_grouped\n",
    " .assign(hour = accidents_grouped['Date and Time'].dt.hour,\n",
    "         weekday = accidents_grouped['Date and Time'].dt.day_name())\n",
    " .query('hour == 6')\n",
    " .nlargest(5, 'Accident Number')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This day was the worst Tuesday in 2019, and is tied for the 4th worst day in the whole year."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
